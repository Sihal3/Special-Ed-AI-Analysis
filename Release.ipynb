{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed for repeatable results, if you wish.\n",
    "# Unneeded due to 50 runs of each model, do not run in most circumstances\n",
    "from numpy.random import seed\n",
    "seed(7567)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(7567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvG_IQay-QX5"
   },
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datagen import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for GPU usage, may need to remove for your setup\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to use LaTeX fonts in matlibplot plots. Currently not working\n",
    "#plt.rcParams.update({\n",
    "#    \"text.usetex\": True,\n",
    "#    \"font.family\": \"sans-serif\",\n",
    "#    \"font.sans-serif\": [\"Computer Modern Sans Serif\"]})\n",
    "# for Palatino and other serif fonts use:\n",
    "#plt.rcParams.update({\n",
    "#    \"text.usetex\": True,\n",
    "#    \"font.family\": \"serif\",\n",
    "#    \"font.serif\": [\"Computer Modern Roman\"],\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#variables to edit and run statistics collection. Cell below must be run first.\n",
    "\n",
    "num_runs=50                                     # number of iterations to run per each configuration, results are averaged out.\n",
    "dataset_sizes=[2500, 5000, 10000, 20000, 50000] # comma-separated values of dataset size to try\n",
    "batch_size=32                                   # batch size of neural network model\n",
    "epochs=[25, 50, 100, 200]                       # comma separated values of training length to try\n",
    "verbose=0                                       # whether to print model training output: 0 for none, 1 for yes, 2 for debug\n",
    "\n",
    "model, data_matrix = collect_statistics(num_runs, dataset_sizes, batch_size, epochs, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function for collecting statistics, cell for running is above.\n",
    "le = LabelEncoder()\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2])], remainder='passthrough')\n",
    "ct2 = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [7])], remainder='passthrough')\n",
    "ct3 = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [18])], remainder='passthrough')\n",
    "scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
    "\n",
    "TARS = tf.keras.models.Sequential()\n",
    "TARS.add(tf.keras.layers.Dense(input_dim = 29 ,units = 50, activation='relu'))\n",
    "TARS.add(tf.keras.layers.Dense(units=30, activation='relu'))\n",
    "TARS.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
    "TARS.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "TARS.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mse'])\n",
    "\n",
    "def collect_statistics(num_runs, num_rowss, batch_size, epochss, verbose):\n",
    "\n",
    "    matrix=[]\n",
    "    for epochs in epochss:\n",
    "        epoch_row=[]\n",
    "        for num_rows in num_rowss:\n",
    "            loss_array=[]\n",
    "            for i in range(num_runs):\n",
    "                dframe = pd.DataFrame()\n",
    "                dframe = datagen(num_rows)\n",
    "                x = dframe.iloc[:, 1:-1].values\n",
    "                y = dframe.iloc[:, -1].values\n",
    "                x[:, 0] = le.fit_transform(x[:,0])\n",
    "                x = np.array(ct.fit_transform(x))\n",
    "                x = np.array(ct2.fit_transform(x))\n",
    "                x = np.array(ct3.fit_transform(x))\n",
    "                scalarX.fit(x)\n",
    "                scalarY.fit(y.reshape(num_rows,1))\n",
    "                x = scalarX.transform(x)\n",
    "                y = scalarY.transform(y.reshape(num_rows,1))\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "                for ix, layer in enumerate(TARS.layers):\n",
    "                    if hasattr(TARS.layers[ix], 'kernel_initializer') and hasattr(TARS.layers[ix], 'bias_initializer'):\n",
    "                        weight_initializer=TARS.layers[ix].kernel_initializer\n",
    "                        bias_initializer=TARS.layers[ix].bias_initializer\n",
    "\n",
    "                        old_weights, old_biases = TARS.layers[ix].get_weights()\n",
    "\n",
    "                        TARS.layers[ix].set_weights([weight_initializer(shape=old_weights.shape), bias_initializer(shape=len(old_biases))])\n",
    "                TARS.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=verbose)\n",
    "                error=TARS.evaluate(x_test, y_test, verbose=0)[0]\n",
    "                #print(f\"Run {i} finished, mean_squared_error: {error}\")\n",
    "                y_pred = TARS.predict(x_test)\n",
    "                y_test = scalarY.inverse_transform(y_test)\n",
    "                y_pred = scalarY.inverse_transform(y_pred)\n",
    "                explained_variance = 1 - np.var(y_test - y_pred)/np.var(y_test)\n",
    "                mse = (sum((y_test-y_pred)**2)/len(y_pred))[0]\n",
    "                loss_array.append([error, explained_variance, mse])\n",
    "            print(f\"Epoch number: {epochs}, num_rows: {num_rows}, model loss: {sum([a[0] for a in loss_array])/len(loss_array)}, explained variance: {sum([a[1] for a in loss_array])/len(loss_array)*100}%, mse: {sum([a[2] for a in loss_array])/len(loss_array)}\")\n",
    "            epoch_row.append(loss_array)\n",
    "        matrix.append(epoch_row)\n",
    "    \n",
    "    return TARS, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTTING VARIABLES/DATA BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot basic diagram of model\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    TARS,\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=False,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=True,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show generated simulated data\n",
    "dframe = datagen(20000)\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of GPA difference variable\n",
    "\n",
    "n, bins, patches = plt.hist(x=dframe['gpadifference'], bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('GPAs')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('GPA Difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-graph matrix display of all data\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "led = LabelEncoder()\n",
    "le_dframe = dframe\n",
    "le_dframe[['gender', 'teacher_cred', 'disability', 'accomadation']] = le_dframe[['gender', 'teacher_cred', 'disability', 'accomadation']].apply(led.fit_transform)\n",
    "scatter_matrix(le_dframe.iloc[:, 1:], alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.query(\"disability == 'Autism'\").groupby(['teacher_cred']).mean()['gpadifference'].plot.bar(data, figsize=(5,5), color = 'red')\n",
    "plt.xlabel('Teacher Credentials')\n",
    "plt.ylabel('Predicted GPA Gain')\n",
    "plt.xticks(rotation=0)\n",
    "plt.title('Predicted GPA Gain by Credentials of Teacher')\n",
    "plt.savefig(\"pattern.svg\", format=\"svg\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1612728018254,
     "user": {
      "displayName": "Nihal Gulati",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXDtWgZ8W0h8IRVQ5-p-nFZu-Z6P-NZa2s6Z9F2w=s64",
      "userId": "09492739079644876153"
     },
     "user_tz": 480
    },
    "id": "UnL3POyQFClA",
    "outputId": "499eccb0-18be-49e4-c1a1-b0b8a866842f"
   },
   "outputs": [],
   "source": [
    "# predict function for individual students \n",
    "# If given all variables, returns expected GPA difference\n",
    "# If not given accomodation, returns predicted most effective accomodation for student and plots it\n",
    "# Need to run cell below to use\n",
    "\n",
    "# input: gender, age, teacher_cred, class size, disability, optional: accommodation \n",
    "predict(np.array([[\"Female\", \"15\", \"Bachelor's\", \"25\", \"ADHD\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function for individual students \n",
    "# If given all variables, returns expected GPA difference\n",
    "# If not given accomodation, returns predicted most effective accomodation and plots\n",
    "\n",
    "def predict(sample_student):\n",
    "    maxdif = -4\n",
    "    bestaccom = \"\"\n",
    "    accomodation_list = [\"Materials in Braille\", \"Text to Speech Devices\", \"Breakout Corner\", \"Use of Toy in Class\", \"Bigger Print Materials\", \"Isolated Workstation\", \"Tutoring Sessions\", \"Book Buddy\", \"Use of Calculator on Tests\", \"AAC Devices\", \"Special Education Classroom\"]\n",
    "    if sample_student[0][-1] in accomodation_list:\n",
    "        accomodation_list = [sample_student[0][-1]]\n",
    "    gpadiffs = []\n",
    "    for i in accomodation_list:\n",
    "        temparray= [[]] \n",
    "        temparray[0] = np.append(sample_student, i)\n",
    "        temparray = np.array(temparray)\n",
    "        temparray[:, 0] = le.transform(temparray[:,0])\n",
    "        temparray = np.array(ct.transform(temparray))\n",
    "        temparray = np.array(ct2.transform(temparray))\n",
    "        temparray = np.array(ct3.transform(temparray))\n",
    "        temparray = scalarX.transform(temparray)\n",
    "        gpadiffs.append(scalarY.inverse_transform(TARS.predict(np.array(temparray)))[0][0])\n",
    "        if (scalarY.inverse_transform(TARS.predict(np.array(temparray))) > maxdif):\n",
    "            maxdif = scalarY.inverse_transform(TARS.predict(np.array(temparray)))\n",
    "            bestaccom = i\n",
    "    \n",
    "    if len(gpadiffs) > 1:\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        ax = fig.add_axes([0,0,1,1])\n",
    "        plt.xticks(rotation=45)\n",
    "        ax.bar(accomodation_list, gpadiffs)\n",
    "        print(f\"{bestaccom} is the predicted best accomodation.\")\n",
    "    else:\n",
    "        print(f\"Predicted GPA Difference for this student is: {gpadiffs[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Compiler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
